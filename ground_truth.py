import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.neural_network import MLPRegressor, MLPClassifier
from econml.dr import DRLearner
import rpy2.robjects as ro
from rpy2.robjects import numpy2ri, default_converter
from rpy2.robjects.packages import importr
from rpy2.robjects.conversion import localconverter
from scipy.spatial.distance import pdist, squareform


ALGORITHMS = ["kmeans-standard", "kmeans-da", 
              "gmm-standard", "gmm-da", 
              "clr-standard", "clr-da",
              "policy_tree", 
              "mst", 
              "dast", 
              "t_learner",
              "x_learner",
              "s_learner",
              "dr_learner",
              "causal_forest"]

class SegmentTrue:
    def __init__(self, segment_id=None, x_mean=None, outcome_type=None,
                 # continuous outcome params
                 alpha=None, beta=None, tau=None, delta=None,
                 # discrete outcome params
                 p=None):
        if outcome_type is None:
            raise ValueError("outcome_type must be specified: 'continuous' or 'discrete'")
        self.outcome_type = outcome_type
        self.x_mean = x_mean
        self.segment_id = segment_id

        if outcome_type == 'discrete':
            assert p is not None, "p must be provided for discrete outcome_type"
            self.p = p          # shape (action_num,): P(Y=1 | D=a, Z=k)
            self.tau = p - p[0] # probability lift vs baseline; tau[0]=0 by definition
            self.action = int(np.argmax(self.p))
            # continuous params not used
            self.alpha = None
            self.beta = None
            self.delta = None

        elif outcome_type == 'continuous':
            assert alpha is not None, "alpha must be provided for continuous outcome_type"
            assert beta is not None, "beta must be provided for continuous outcome_type"
            assert tau is not None, "tau must be provided for continuous outcome_type"
            self.alpha = alpha
            self.beta = beta    # shape (d,)
            self.tau = tau      # shape (action_num,): tau[0]=0, tau[a] = effect of action a
            self.delta = delta  # shape (action_num, d) or None
            self.action = int(np.argmax(self.tau))
            # discrete params not used
            self.p = None

        else:
            raise ValueError(f"Unknown outcome_type: '{outcome_type}'. Must be 'continuous' or 'discrete'.")

    def generate_outcome(self, x, D_i, noise_std, signal_d):
        """Stochastic outcome sample — use for data generation only."""
        if self.outcome_type == 'discrete':
            # Y ~ Bernoulli(p[D_i]); x and noise_std are not used
            return int(np.random.binomial(1, self.p[int(D_i)]))
        elif self.outcome_type == 'continuous':
            noise = np.random.normal(0, noise_std)
            # y = alpha + beta @ x + tau[D_i] + delta[D_i] @ x + noise
            base = self.alpha + self.beta[:signal_d] @ x[:signal_d] + self.tau[int(D_i)]
            if self.delta is not None:
                interaction = self.delta[int(D_i), :signal_d] @ x[:signal_d]
                return base + interaction + noise
            return base + noise

    def expected_outcome(self, x, D_i, signal_d):
        """Deterministic expected outcome — use for profit evaluation and oracle metrics.

        Continuous: E[Y | x, D_i] = alpha + beta @ x + tau[D_i] + delta[D_i] @ x  (no noise)
        Discrete:   E[Y | D_i]    = p[D_i]  (expected probability of Y=1; x not used)
        """
        if self.outcome_type == 'discrete':
            return float(self.p[int(D_i)])
        elif self.outcome_type == 'continuous':
            base = self.alpha + self.beta[:signal_d] @ x[:signal_d] + self.tau[int(D_i)]
            if self.delta is not None:
                interaction = self.delta[int(D_i), :signal_d] @ x[:signal_d]
                return base + interaction
            return base
        

        
class SegmentEstimate:
    def __init__(self, est_tau, est_action, segment_id=None):
        self.est_tau = est_tau
        self.est_action = est_action  # 0 or 1
        self.segment_id = segment_id


class Customer_pilot:
    def __init__(self, x, D_i, y, true_segment: SegmentTrue,  customer_id=None):
        self.x = x
        self.D_i = D_i
        self.true_segment = true_segment
        self.y = y
        self.est_segment = {algo: None for algo in ALGORITHMS}

        self.customer_id = customer_id

class Customer_implement:
    def __init__(self, x, true_segment: SegmentTrue, noise_std, signal_d):
        self.x = x
        self.true_segment = true_segment
        self.noise_std = noise_std
        self.signal_d = signal_d
        
        self.est_segment = {algo: None for algo in ALGORITHMS}
        
    def evaluate_profits(self, algo, implement_action=None):
        """Evaluate deterministic expected profit under the algorithm's recommended action.

        Uses expected_outcome (not generate_outcome) so the result is always deterministic:
          - continuous: E[Y] = alpha + beta @ x + tau[a] + delta[a] @ x  (no noise)
          - discrete:   E[Y] = p[a]  (expected success probability)
        """
        if implement_action is None:
            if self.est_segment[algo].est_action != 404:
                self.implement_action = self.est_segment[algo].est_action
            else:
                self.implement_action = self.true_segment.action  # 404 fallback
        else:
            self.implement_action = implement_action

        self.y = self.true_segment.expected_outcome(self.x, self.implement_action, self.signal_d)
        return self.y
        
# ----------------------------------------
# Population Simulator
# ----------------------------------------

class PopulationSimulator:
    def __init__(self, N_total_pilot_customers, N_total_implement_customers, d, K, disturb_covariate_noise, param_range, DR_generation_method, partial_x, action_num, X_mean_vectors=None, X_noise_std_scale=None, Y_noise_std_scale=None, disallowed_ball_radius=None, outcome_type=None):
        self.N_total_pilot_customers = N_total_pilot_customers
        self.N_total_implement_customers = N_total_implement_customers
        self.d = d
        self.K = K
        self.action_num = action_num
        if outcome_type is None:
            raise ValueError("outcome_type is required. Please specify 'continuous' or 'discrete'.")
        self.outcome_type = outcome_type

        self.param_range = param_range
        self.disturb_covariate_noise = disturb_covariate_noise
        self.signal_d = d if partial_x == 1 else max(1, int(d * partial_x))
        self.disturb_d = d - self.signal_d
        self.disallowed_ball_radius = disallowed_ball_radius if disallowed_ball_radius is not None else 0

        self.true_segments = self._init_true_segments(X_mean_vectors)
        
        # Compute signal_covariate_noise based on X_noise_std_scale (required parameter)
        if X_noise_std_scale is None:
            raise ValueError("X_noise_std_scale is required. Please provide a scale factor for within-cluster covariate noise.")
        
        if self.K <= 1:
            raise ValueError(f"Cannot use X_noise_std_scale with K={self.K}. Need at least 2 clusters to compute average distance.")
        
        mean_vectors_signal = np.array([seg.x_mean[:self.signal_d] for seg in self.true_segments])
        pairwise_distances = pdist(mean_vectors_signal, metric='euclidean')
        
        if len(pairwise_distances) == 0:
            raise ValueError(f"No pairwise distances computed for K={self.K} clusters.")
        
        avg_distance = np.mean(pairwise_distances)
        self.signal_covariate_noise = X_noise_std_scale * avg_distance
        print(f"Computed X_covariate_noise: {self.signal_covariate_noise:.4f} (scale={X_noise_std_scale}, avg_distance={avg_distance:.4f})")
        
        self._adjust_adjacent_cluster_tau()
        
        # noise_std: only meaningful for continuous; discrete uses Bernoulli randomness
        if outcome_type == 'continuous':
            if Y_noise_std_scale is None:
                raise ValueError("Y_noise_std_scale is required for continuous outcome_type.")
            tau_values = np.array([seg.tau[1:] for seg in self.true_segments]).flatten()
            avg_tau_magnitude = np.mean(np.abs(tau_values)) if len(tau_values) > 0 else 1.0
            self.noise_std = Y_noise_std_scale * avg_tau_magnitude
            print(f"Computed Y_noise_std: {self.noise_std:.4f} (scale={Y_noise_std_scale}, avg_|tau|={avg_tau_magnitude:.4f})")
        else:
            # discrete: no Gaussian noise; noise_std is irrelevant but stored as 0
            self.noise_std = 0.0
            if Y_noise_std_scale is not None:
                print("Note: Y_noise_std_scale is ignored for discrete outcome_type.")
        
        self.pilot_customers = self._generate_pilot_customers()
        self.implement_customers = self._generate_implement_customers()
        
        self.est_segments_list = {algo: [] for algo in ALGORITHMS}
        
        # Store DR generation method for later use
        self.DR_generation_method = DR_generation_method
        self.gamma_train = None  # Will be computed after train/val split
        self.gamma_val = None    # Will be computed after train/val split
        
        self.train_customers, self.val_customers, self.train_indices, self.val_indices = None, None, None, None
        
            
    def _init_true_segments(self, X_mean_vectors):
        pr = self.param_range  # alias for convenience
        true_segments = []
        
        def _make_segment_params(k):
            """Sample outcome parameters for one segment based on outcome_type."""
            if self.outcome_type == 'discrete':
                # p[a] ~ Uniform(p_range) for each action
                p_vec = np.array([np.random.uniform(*pr["p"]) for _ in range(self.action_num)])
                return dict(alpha=None, beta=None, tau=None, delta=None, p=p_vec)
            else:
                alpha = np.random.uniform(*pr["alpha"])
                beta = np.random.uniform(*pr["beta"], size=self.d)
                tau_vec = np.zeros(self.action_num)
                for a in range(1, self.action_num):
                    tau_vec[a] = np.random.uniform(*pr["tau"])
                if pr.get("delta") is not None:
                    delta_mat = np.zeros((self.action_num, self.d))
                    for a in range(1, self.action_num):
                        delta_mat[a] = np.random.uniform(*pr["delta"], size=self.d)
                else:
                    delta_mat = None
                return dict(alpha=alpha, beta=beta, tau=tau_vec, delta=delta_mat, p=None)

        # If generating mean vectors, use minimum distance constraint
        if X_mean_vectors is None:
            # Calculate a reasonable minimum distance based on space size
            space_range = pr["x_mean"][1] - pr["x_mean"][0]
            min_distance = (space_range / (self.K ** (1.0 / self.d))) * self.disallowed_ball_radius
            print(f"Generating mean vectors with minimum distance: {min_distance:.2f}")
            
            generated_means = []
            for k in range(self.K):
                params = _make_segment_params(k)
                
                # Generate x_mean with minimum distance constraint
                max_attempts = 100
                for attempt in range(max_attempts):
                    x_mean_candidate = np.random.uniform(*pr["x_mean"], size=self.d)
                    
                    if len(generated_means) == 0:
                        x_mean = x_mean_candidate
                        break
                    
                    distances = [np.linalg.norm(x_mean_candidate - existing) 
                                for existing in generated_means]
                    min_dist_to_existing = min(distances)
                    
                    if min_dist_to_existing >= min_distance:
                        x_mean = x_mean_candidate
                        break
                    
                    if attempt == max_attempts - 1:
                        print(f"⚠️  Warning: Could not find mean vector for cluster {k} satisfying min distance.")
                        print(f"   Using best candidate with distance {min_dist_to_existing:.2f}")
                        x_mean = x_mean_candidate
                
                generated_means.append(x_mean)
                true_segments.append(SegmentTrue(
                    segment_id=k, x_mean=x_mean, outcome_type=self.outcome_type,
                    alpha=params["alpha"], beta=params["beta"], tau=params["tau"],
                    delta=params["delta"], p=params["p"]))
        else:
            # Use provided mean vectors
            for k in range(self.K):
                params = _make_segment_params(k)
                x_mean = X_mean_vectors[k]
                true_segments.append(SegmentTrue(
                    segment_id=k, x_mean=x_mean, outcome_type=self.outcome_type,
                    alpha=params["alpha"], beta=params["beta"], tau=params["tau"],
                    delta=params["delta"], p=params["p"]))
        
        return true_segments
    
    def _adjust_adjacent_cluster_tau(self):
        """
        Make sure "adjacent but not overlapping" clusters have opposite treatment effect signs.
        Adjacent means: distance ≈ 1-3 sigma (touching at boundaries, not fully overlapping).
        This creates a challenging scenario for algorithms.
        """
        if self.K < 2:
            return
        
        # Extract mean vectors (only signal dimensions matter for clustering)
        mean_vectors_signal = np.array([seg.x_mean[:self.signal_d] for seg in self.true_segments])
        
        # Compute pairwise distances in signal space
        dist_matrix = squareform(pdist(mean_vectors_signal, metric='euclidean'))
        
        # Set diagonal to infinity to exclude self-distances
        np.fill_diagonal(dist_matrix, np.inf)
        
        # Define "adjacent" as distance in range [lower_bound, upper_bound]
        # This means clusters touch at boundaries but don't heavily overlap
        sigma = self.signal_covariate_noise
        lower_bound = 1.0 * sigma  # Closer than this = too much overlap
        upper_bound = 4.0 * sigma  # Farther than this = well separated
        
        # Find pairs in the "adjacent" range
        adjacent_pairs = []
        for i in range(self.K):
            for j in range(i+1, self.K):
                dist = dist_matrix[i, j]
                if lower_bound <= dist <= upper_bound:
                    adjacent_pairs.append((i, j, dist))
        
        if adjacent_pairs:
            # Pick the pair with distance closest to 2*sigma (sweet spot for "touching")
            target_dist = 2.0 * sigma
            best_pair = min(adjacent_pairs, key=lambda x: abs(x[2] - target_dist))
            idx1, idx2, dist = best_pair
            
            action1_before = self.true_segments[idx1].action
            action2_before = self.true_segments[idx2].action
            
            # If same best action, fix seg2 to have a different best action
            if action1_before == action2_before:
                seg2 = self.true_segments[idx2]
                if self.outcome_type == 'discrete':
                    # Re-sample p for seg2 until best action differs
                    pr = self.param_range
                    for _ in range(1000):
                        p_new = np.array([np.random.uniform(*pr["p"]) for _ in range(self.action_num)])
                        if int(np.argmax(p_new)) != action1_before:
                            seg2.p = p_new
                            seg2.tau = p_new - p_new[0]
                            seg2.action = int(np.argmax(p_new))
                            break
                else:
                    seg2.tau[1:] = -seg2.tau[1:]
                    seg2.action = int(np.argmax(seg2.tau))

                action2_after = self.true_segments[idx2].action
                print(f"⚠️  Adjacent clusters (segments {idx1} and {idx2}) had same best action ({action1_before}).")
                print(f"   Distance: {dist:.2f}, sigma={sigma:.2f}")
                print(f"   Fixed seg {idx2}. New best action: {action2_after}")
            else:
                print(f"✓ Adjacent clusters (segments {idx1} and {idx2}) already have different best actions.")
                print(f"   Distance: {dist:.2f}, sigma={sigma:.2f}")
                print(f"   Seg{idx1}: action={action1_before}, Seg{idx2}: action={action2_before}")

    def _generate_pilot_customers(self):
        pilot_customers = []

        cov_signal = np.eye(self.signal_d) * (self.signal_covariate_noise ** 2) 
        if self.disturb_d > 0:
            # === 1️⃣ Generate adversarial noise clusters ===
            cov_noise = np.eye(self.disturb_d) * (self.disturb_covariate_noise ** 2)
            N_noise_clusters = max(1, int(np.random.uniform(self.K - 5, self.K + 5)))
            noise_cluster_means = [
                np.random.uniform(*self.param_range["x_mean"], size=self.disturb_d)
                for _ in range(N_noise_clusters)
            ]
            noise_cluster_labels = np.random.randint(
                0, N_noise_clusters, size=self.N_total_pilot_customers
            )
        for i in range(self.N_total_pilot_customers):
            # 2️⃣ Choose true segment for signal part
            segment = np.random.choice(self.true_segments)

            # 3️⃣ Signal features: cluster-dependent
            # Only use first signal_d dimensions of x_mean
            x_signal = np.random.multivariate_normal(mean=segment.x_mean[:self.signal_d], cov=cov_signal)

            # 4️⃣ Disturbing features: come from random noise cluster
            if self.disturb_d > 0:
                w = noise_cluster_labels[i]
                x_noise = np.random.multivariate_normal(mean=noise_cluster_means[w], cov=cov_noise)
                x_full = np.concatenate([x_signal, x_noise])

            else:
                x_full = x_signal

            # 6️⃣ Outcome
            D_i = np.random.choice(self.action_num)  # Randomly assign action from 0 to action_num-1
            y = segment.generate_outcome(x_full, D_i, self.noise_std, self.signal_d)

            # 7️⃣ Save
            cust = Customer_pilot(x_full, D_i, y, segment, customer_id=i)
            pilot_customers.append(cust)

        return pilot_customers
     
    def _generate_implement_customers(self):
        implement_customers = []

        # --- Signal and noise covariance ---
        cov_signal = np.eye(self.signal_d) * (self.signal_covariate_noise ** 2)
        

        # === 1️⃣  noise clusters ===
        if self.disturb_d > 0:
            cov_noise = np.eye(self.disturb_d) * (self.disturb_covariate_noise ** 2)
            N_noise_clusters = max(1, int(np.random.uniform(self.K - 5, self.K + 5)))

            # Randomly position these noise cluster means
            noise_cluster_means = [
                np.random.uniform(*self.param_range["x_mean"], size=self.disturb_d)
                for _ in range(N_noise_clusters)
            ]

            # Assign each implement customer to a random noise cluster
            noise_cluster_labels = np.random.randint(
                0, N_noise_clusters, size=self.N_total_implement_customers
            )

        for i in range(self.N_total_implement_customers):
            # 2️⃣ Choose true signal segment (same as before)
            segment = np.random.choice(self.true_segments)

            # 3️⃣ Signal part: segment-dependent
            # Only use first signal_d dimensions of x_mean
            x_signal = np.random.multivariate_normal(mean=segment.x_mean[:self.signal_d], cov=cov_signal)

            # 4️⃣ Noise part: from an unrelated latent noise cluster
            if self.disturb_d > 0:
                w = noise_cluster_labels[i]
                x_noise = np.random.multivariate_normal(mean=noise_cluster_means[w], cov=cov_noise)

                # 5️⃣ Combine signal + noise
                x_full = np.concatenate([x_signal, x_noise])
            else:
                x_full = x_signal

            # 6️⃣ Create Customer_implement object
            cust = Customer_implement(x_full, segment, self.noise_std, self.signal_d)
            implement_customers.append(cust)

        return implement_customers





    
    def compute_gamma_scores(self, method, train_customers, val_customers):
        """
        Compute doubly robust (DR) scores for both training and validation customers.
        
        Strategy:
        1. Fit outcome models on TRAIN data
        2. Predict and compute DR scores on both TRAIN and VAL data
        
        Parameters:
        method : str
            Method for computing gamma ('reg', 'mlp', or 'forest')
        train_customers : list
            Training customers
        val_customers : list
            Validation customers (can be empty)
            
        Returns:
        Gamma_train : array-like, shape (N_train, 2)
            DR scores for training customers
        Gamma_val : array-like, shape (N_val, 2) or None
            DR scores for validation customers (None if val_customers is empty)
        """
        # Extract train data
        X_train = np.array([cust.x for cust in train_customers])
        D_train = np.array([cust.D_i for cust in train_customers])
        Y_train = np.array([cust.y for cust in train_customers])
        
        # Extract validation data (handle empty case)
        if len(val_customers) > 0:
            X_val = np.array([cust.x for cust in val_customers])
            D_val = np.array([cust.D_i for cust in val_customers])
            Y_val = np.array([cust.y for cust in val_customers])
        else:
            X_val = None
            D_val = None
            Y_val = None
        
        # Empirical propensity scores: e[a] = P(D=a) for each action a
        n_actions = self.action_num
        e = np.array([np.mean(D_train == a) for a in range(n_actions)])
        e = np.clip(e, 1e-6, 1.0)  # avoid division by zero

        is_discrete = self.outcome_type == 'discrete'

        def _predict_mu(model, X):
            """Return E[Y|X] - for classifiers, return P(Y=1|X)."""
            if hasattr(model, 'predict_proba'):
                return model.predict_proba(X)[:, 1]
            return model.predict(X)

        def _compute_gamma(X, D, Y, models):
            """
            DR score for each action a:
            Gamma[i, a] = mu_a(X_i) + (1/e[a]) * 1[D_i == a] * (Y_i - mu_a(X_i))
            Works for both continuous (mu_a = E[Y]) and discrete (mu_a = P(Y=1)).
            """
            N = X.shape[0]
            Gamma = np.zeros((N, n_actions))
            for a in range(n_actions):
                mu_a = _predict_mu(models[a], X)
                indicator = (D == a).astype(float)
                Gamma[:, a] = mu_a + (indicator / e[a]) * (Y - mu_a)
            return Gamma

        if method == "reg":
            models = {}
            for a in range(n_actions):
                X_a = X_train[D_train == a]
                Y_a = Y_train[D_train == a]
                if len(X_a) == 0:
                    raise ValueError(f"No training samples for action {a}. Cannot fit outcome model.")
                if is_discrete:
                    m = LogisticRegression(max_iter=1000)
                else:
                    m = LinearRegression()
                m.fit(X_a, Y_a)
                models[a] = m

            Gamma_train = _compute_gamma(X_train, D_train, Y_train, models)
            Gamma_val = _compute_gamma(X_val, D_val, Y_val, models) if X_val is not None and len(X_val) > 0 else None

        elif method == "mlp":
            models = {}
            for a in range(n_actions):
                X_a = X_train[D_train == a]
                Y_a = Y_train[D_train == a]
                if len(X_a) == 0:
                    raise ValueError(f"No training samples for action {a}. Cannot fit outcome model.")
                if is_discrete:
                    m = MLPClassifier(hidden_layer_sizes=(64, 32), activation='relu', max_iter=10000)
                else:
                    m = MLPRegressor(hidden_layer_sizes=(64, 32), activation='relu', max_iter=10000)
                m.fit(X_a, Y_a)
                models[a] = m

            Gamma_train = _compute_gamma(X_train, D_train, Y_train, models)
            Gamma_val = _compute_gamma(X_val, D_val, Y_val, models) if X_val is not None and len(X_val) > 0 else None

        elif method == "lightgbm":
            try:
                from lightgbm import LGBMRegressor, LGBMClassifier
            except ImportError:
                raise ImportError("lightgbm is not installed. Run: pip install lightgbm")
            models = {}
            for a in range(n_actions):
                X_a = X_train[D_train == a]
                Y_a = Y_train[D_train == a]
                if len(X_a) == 0:
                    raise ValueError(f"No training samples for action {a}. Cannot fit outcome model.")
                if is_discrete:
                    m = LGBMClassifier(n_estimators=100, verbose=-1)
                else:
                    m = LGBMRegressor(n_estimators=100, verbose=-1)
                m.fit(X_a, Y_a)
                models[a] = m

            Gamma_train = _compute_gamma(X_train, D_train, Y_train, models)
            Gamma_val = _compute_gamma(X_val, D_val, Y_val, models) if X_val is not None and len(X_val) > 0 else None

        elif method == "random_forest":
            from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
            models = {}
            for a in range(n_actions):
                X_a = X_train[D_train == a]
                Y_a = Y_train[D_train == a]
                if len(X_a) == 0:
                    raise ValueError(f"No training samples for action {a}. Cannot fit outcome model.")
                if is_discrete:
                    m = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)
                else:
                    m = RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=42)
                m.fit(X_a, Y_a)
                models[a] = m

            Gamma_train = _compute_gamma(X_train, D_train, Y_train, models)
            Gamma_val = _compute_gamma(X_val, D_val, Y_val, models) if X_val is not None and len(X_val) > 0 else None

        elif method == "xgboost":
            try:
                from xgboost import XGBRegressor, XGBClassifier
            except ImportError:
                raise ImportError("xgboost is not installed. Run: pip install xgboost")
            models = {}
            for a in range(n_actions):
                X_a = X_train[D_train == a]
                Y_a = Y_train[D_train == a]
                if len(X_a) == 0:
                    raise ValueError(f"No training samples for action {a}. Cannot fit outcome model.")
                if is_discrete:
                    m = XGBClassifier(n_estimators=100, verbosity=0, random_state=42)
                else:
                    m = XGBRegressor(n_estimators=100, verbosity=0, random_state=42)
                m.fit(X_a, Y_a)
                models[a] = m

            Gamma_train = _compute_gamma(X_train, D_train, Y_train, models)
            Gamma_val = _compute_gamma(X_val, D_val, Y_val, models) if X_val is not None and len(X_val) > 0 else None

        else:
            raise ValueError(f"Unknown DR generation method: '{method}'. "
                             f"Choose from: reg, mlp, lightgbm, random_forest, xgboost")

        return Gamma_train, Gamma_val


    def split_pilot_customers_into_train_and_validate(self, train_frac=0.8):
        indices = np.arange(self.N_total_pilot_customers)

        split = int(self.N_total_pilot_customers * train_frac)
        self.train_indices, self.val_indices = indices[:split], indices[split:]

        self.train_customers = [self.pilot_customers[i] for i in self.train_indices]
        self.val_customers = [self.pilot_customers[i] for i in self.val_indices]
        
        # Compute gamma scores
        if train_frac < 1.0:
            # Normal case: have both train and val
            self.gamma_train, self.gamma_val = self.compute_gamma_scores(
                self.DR_generation_method, self.train_customers, self.val_customers
            )
        else:
            # train_frac=1.0: all data is training, no validation
            # Still compute gamma_train (some algorithms need it), but gamma_val = None
            self.gamma_train, self.gamma_val = self.compute_gamma_scores(
                self.DR_generation_method, self.train_customers, []  # Empty val_customers
            )

    def to_dataframe(self):
        data = []
        for cust in self.pilot_customers:
            row = {
                "customer_id": cust.customer_id,
                "true_segment_id": cust.true_segment.segment_id,
                "D_i": cust.D_i,
                "outcome": cust.y,
            }

            # 动态添加每个算法的 est_segment_id
            for algo in ALGORITHMS:
                row[f"{algo}_est_segment_id"] = (
                    cust.est_segment[algo].segment_id if cust.est_segment[algo] is not None else None
                )
            # 添加特征向量 x
            for j in range(self.d):
                row[f"x_{j}"] = cust.x[j]

            data.append(row)

        return pd.DataFrame(data)









